{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 证明PCA的目标中，最小化重构误差和最大化方差是等价的\n",
    "\n",
    "#### 1.1. 最小化重构误差目标\n",
    "\n",
    "PCA 的目标是找到一个投影矩阵 $\\mathbf{P}$，使得投影后的数据 $\\mathbf{Y} =\\mathbf{P} \\mathbf{X}$最大化其方差，同时最小化重构误差，因此PCA的目标是：\n",
    "\n",
    "$$\\min_P \\|\\mathbf{X} - \\mathbf{P}^T \\mathbf{Y} \\|^2_F, \\quad \\operatorname{s.t.} \\mathbf{P}^T\\mathbf{P}=\\mathbf{I}$$\n",
    "\n",
    "其中$P$是正定矩阵，因此 $\\mathbf{P}^T \\mathbf{P} = \\mathbf{I}$ 可以逆推出重构后的数据为 $\\mathbf{X}_{recovery} = \\mathbf{P}^T \\mathbf{Y}$。\n",
    "\n",
    "#### 1.2. 最大化方差目标\n",
    "\n",
    "设$\\bf X$是去中心化了的数据，则协方差矩阵表示为\n",
    "\n",
    "$$ \\mathbf{S} = \\frac{1}{n} \\mathbf{X}^T \\mathbf{X} $$\n",
    "\n",
    "投影后的数据协方差矩阵为：\n",
    "\n",
    "$$ \\mathbf{S}_{\\text{Y}} =\\frac{1}{n} \\mathbf{Y}^T \\mathbf{Y}= \\frac{1}{n} (\\mathbf{P} \\mathbf{X})^T (\\mathbf{P} \\mathbf{X}) = \\mathbf{P} \\mathbf{S} \\mathbf{P}^T $$\n",
    "\n",
    "因此 PCA 的目标是 \n",
    "$$\\max_P \\operatorname{tr}(\\mathbf{P} \\mathbf{S} \\mathbf{P}^T),\\quad  \\operatorname{s.t.} \\mathbf{P}^T\\mathbf{P}=\\mathbf{I}$$\n",
    "\n",
    "#### 1.3. 等价性证明\n",
    "对于最小化重构误差目标：\n",
    "\n",
    "$$ \\|\\mathbf{X} - \\mathbf{P}^T \\mathbf{Y}\\|_F^2 $$\n",
    "展开范数：\n",
    "$$ \\|\\mathbf{X} - \\mathbf{P}^T \\mathbf{Y}\\|_F^2 = \\operatorname{tr}((\\mathbf{X}^T\\mathbf{X} -2 \\mathbf{X}^T \\mathbf{P}^T \\mathbf{Y}  +\\mathbf{Y}^T \\mathbf{P} \\mathbf{P}^T \\mathbf{Y})$$\n",
    "由于 $\\mathbf{P}^T \\mathbf{P}=\\mathbf{P} \\mathbf{P}^T = \\mathbf{I}$，变换可得：\n",
    "$$ \\operatorname{tr}(\\mathbf{X}^T \\mathbf{X}) - 2 \\operatorname{tr}(\\mathbf{Y}^T \\mathbf{Y}) + \\operatorname{tr}( \\mathbf{Y}^T \\mathbf{Y}) =\\operatorname{tr}( \\mathbf{X}^T \\mathbf{X})-\\operatorname{tr}( \\mathbf{Y}^T \\mathbf{Y})$$\n",
    "其中，$\\operatorname{tr}(\\mathbf{X}^T \\mathbf{X})$ 是常数。因此，最小化重构误差等价于最大化 $\\operatorname{tr}(\\mathbf{Y}^T \\mathbf{Y})$，即最大化 ：\n",
    "$$\\operatorname{tr}(\\mathbf{Y}^T \\mathbf{Y})=\\operatorname{tr}[(\\mathbf{PX})^T (\\mathbf{PX})]=\\operatorname{tr}(\\mathbf{PX}\\mathbf{X}^T\\mathbf{P}^T)=n\\operatorname{tr}(\\mathbf{P} \\mathbf{S} \\mathbf{P}^T)$$\n",
    "因此等效于：\n",
    "\n",
    "$$\\max_P \\operatorname{tr}(\\mathbf{P} \\mathbf{S} \\mathbf{P}^T),\\quad  \\operatorname{s.t.} \\mathbf{P}^T\\mathbf{P}=\\mathbf{I}$$\n",
    "\n",
    "#### 1.4. 求解推导\n",
    "\n",
    "对于目标函数：\n",
    "\n",
    "$$\\max_P \\operatorname{tr}(\\mathbf{P} \\mathbf{S} \\mathbf{P}^T),\\quad  \\operatorname{s.t.} \\mathbf{P}^T\\mathbf{P}=\\mathbf{I}$$\n",
    "\n",
    "用拉格朗日乘子法求解，构造拉格朗日函数：\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{P}, \\mathbf{\\Lambda}) = \\operatorname{tr}(\\mathbf{P} \\mathbf{S} \\mathbf{P}^T) + \\operatorname{tr}(\\mathbf{\\Lambda} (\\mathbf{I}-\\mathbf{P}^T\\mathbf{P}))$$\n",
    "\n",
    "其中 $\\mathbf{\\Lambda}$ 是拉格朗日乘子矩阵，表示为：\n",
    "\n",
    "$$\\mathbf{\\Lambda} = \\begin{bmatrix} \\lambda_1 & 0 & \\cdots & 0 \\\\ 0 & \\lambda_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\lambda_n \\end{bmatrix}$$\n",
    "\n",
    "对 $\\mathbf{P}$ 求导，令导数为 $0$，得到：\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{P}} = 2 \\mathbf{S} \\mathbf{P} - 2 \\mathbf{P} \\mathbf{\\Lambda} = 0$$\n",
    "\n",
    "解得：\n",
    "\n",
    "$$\\mathbf{S} \\mathbf{P} = \\mathbf{P} \\mathbf{\\Lambda}$$\n",
    "\n",
    "其中$ \\bf{S} = \\frac{1}{n} \\bf{X}^T \\bf{X}$，而 $\\mathbf{P}$ 是 $\\mathbf{S}$ 的特征向量，$\\mathbf{\\Lambda}$ 是特征值对角矩阵。\n",
    "\n",
    "或表示为：\n",
    "\n",
    "$$\\mathbf{S} \\mathbf{w}_i = \\lambda_i \\mathbf{w}_i$$\n",
    "\n",
    "其中 $\\mathbf{w}_i$ 是 $\\mathbf{P}$ 的列向量，表示为 $\\mathbf{P} = [\\mathbf{w}_1, \\mathbf{w}_2, \\cdots, \\mathbf{w}_n]$, $\\lambda_i$ 是对应的特征值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 证明 $S_t = S_w + S_b$\n",
    "\n",
    "\n",
    "- **算法关键**：$\\mathbf{x}_i - \\mathbf{m} = (\\mathbf{x}_i - \\mathbf{m}_j) + (\\mathbf{m}_j - \\mathbf{m})$\n",
    "\n",
    "- **算法关键**：$\\mathbf{x}_i - \\mathbf{m} = (\\mathbf{x}_i - \\mathbf{m}_j) + (\\mathbf{m}_j - \\mathbf{m})$\n",
    "\n",
    "- **算法关键**：$\\mathbf{x}_i - \\mathbf{m} = (\\mathbf{x}_i - \\mathbf{m}_j) + (\\mathbf{m}_j - \\mathbf{m})$\n",
    "\n",
    "\n",
    "我们将证明总散布矩阵 $ S_t $ 可以分解为类内散布矩阵 $ S_w $ 和类间散布矩阵 $ S_b $。\n",
    "\n",
    "- 总散布矩阵 $ S_t $：\n",
    "  $$\n",
    "  S_t = \\sum_{i=1}^n (\\mathbf{x}_i - \\mathbf{m})(\\mathbf{x}_i - \\mathbf{m})^T\n",
    "  $$\n",
    "  其中，$ \\mathbf{x}_i $ 是第 $ i $ 个样本，$ \\mathbf{m} $ 是所有样本的均值：\n",
    "  $$\n",
    "  \\mathbf{m} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i\n",
    "  $$\n",
    "- 类内散布矩阵 $ S_w $：\n",
    "  $$\n",
    "  S_w = \\sum_{j=1}^c \\sum_{\\mathbf{x}_i \\in C_j} (\\mathbf{x}_i - \\mathbf{m}_j)(\\mathbf{x}_i - \\mathbf{m}_j)^T\n",
    "  $$\n",
    "  其中，$ C_j $ 表示第 $ j $ 类，$ \\mathbf{m}_j $ 是第 $ j $ 类的均值：\n",
    "  $$\n",
    "  \\mathbf{m}_j = \\frac{1}{n_j} \\sum_{\\mathbf{x}_i \\in C_j} \\mathbf{x}_i\n",
    "  $$\n",
    "- 类间散布矩阵 $ S_b $：\n",
    "  $$\n",
    "  S_b = \\sum_{j=1}^c n_j (\\mathbf{m}_j - \\mathbf{m})(\\mathbf{m}_j - \\mathbf{m})^T\n",
    "  $$\n",
    "\n",
    "为了证明 $ S_t = S_w + S_b $，我们从 $ S_t $ 的定义出发，将其重新分解。\n",
    "\n",
    "\n",
    "首先，将总散布矩阵 $ S_t $ 重新写成类内散布和类间散布的形式：\n",
    "$$\n",
    "S_t = \\sum_{i=1}^n (\\mathbf{x}_i - \\mathbf{m})(\\mathbf{x}_i - \\mathbf{m})^T\n",
    "$$\n",
    "我们可以把每个样本的偏差 $ \\mathbf{x}_i - \\mathbf{m} $ 拆解为两部分：样本相对于其类均值的偏差 $ \\mathbf{x}_i - \\mathbf{m}_j $ 和类均值相对于总体均值的偏差 $ \\mathbf{m}_j - \\mathbf{m} $：\n",
    "$$\n",
    "\\mathbf{x}_i - \\mathbf{m} = (\\mathbf{x}_i - \\mathbf{m}_j) + (\\mathbf{m}_j - \\mathbf{m})\n",
    "$$\n",
    "代入到总散布矩阵 $ S_t $ 中：\n",
    "$$\n",
    "S_t = \\sum_{i=1}^n \\left[ (\\mathbf{x}_i - \\mathbf{m}_j) + (\\mathbf{m}_j - \\mathbf{m}) \\right] \\left[ (\\mathbf{x}_i - \\mathbf{m}_j) + (\\mathbf{m}_j - \\mathbf{m}) \\right]^T\n",
    "$$\n",
    "展开并分离项：\n",
    "$$\n",
    "S_t = \\sum_{i=1}^n (\\mathbf{x}_i - \\mathbf{m}_j)(\\mathbf{x}_i - \\mathbf{m}_j)^T + \\sum_{i=1}^n (\\mathbf{m}_j - \\mathbf{m})(\\mathbf{m}_j - \\mathbf{m})^T + \\sum_{i=1}^n (\\mathbf{x}_i - \\mathbf{m}_j)(\\mathbf{m}_j - \\mathbf{m})^T + \\sum_{i=1}^n (\\mathbf{m}_j - \\mathbf{m})(\\mathbf{x}_i - \\mathbf{m}_j)^T\n",
    "$$\n",
    "\n",
    "注意到对于每个类$j$中都有：\n",
    "$$\n",
    "\\sum_{\\mathbf{x}\\in C_j} (\\mathbf{x}-\\mathbf{m}_j)=(n_j\\mathbf{m}_j-n_j\\mathbf{m}_j)=0,\\quad \\forall j\n",
    "$$\n",
    "\n",
    "因此只剩下类内和类间散布的部分：\n",
    "$$\n",
    "S_t = \\sum_{i=1}^n (\\mathbf{x}_i - \\mathbf{m}_j)(\\mathbf{x}_i - \\mathbf{m}_j)^T + \\sum_{i=1}^n (\\mathbf{m}_j - \\mathbf{m})(\\mathbf{m}_j - \\mathbf{m})^T\n",
    "$$\n",
    "类内散布矩阵 $ S_w $ 是所有类内散布的和：\n",
    "$$\n",
    "S_w = \\sum_{j=1}^c \\sum_{\\mathbf{x}_i \\in C_j} (\\mathbf{x}_i - \\mathbf{m}_j)(\\mathbf{x}_i - \\mathbf{m}_j)^T\n",
    "$$\n",
    "类间散布矩阵 $ S_b $ 是所有类间散布的和：\n",
    "$$\n",
    "S_b = \\sum_{j=1}^c n_j (\\mathbf{m}_j - \\mathbf{m})(\\mathbf{m}_j - \\mathbf{m})^T\n",
    "$$\n",
    "结合上述等式，我们得到：\n",
    "$$\n",
    "S_t = S_w + S_b\n",
    "$$\n",
    "因此，总散布矩阵 $ S_t $ 可以分解为类内散布矩阵 $ S_w $ 和类间散布矩阵 $ S_b $ 的和。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

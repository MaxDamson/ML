{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 介绍\n",
    "\n",
    "- **LDA劣势和提出背景**：\n",
    "    - LDA忽略了数据的局部结构，即数据点之间的邻近关系没有被充分利用。\n",
    "    - 邻近点之间的判别分析容易受到原始数据空间中噪声的影响。\n",
    "  \n",
    "- **LADA的优化目标和约束条件**：<mark>学习数据的代表性子空间</mark>，具体有\n",
    "  - **优化目标**：通过学习一个最优变换矩阵 $W$，在保证变换矩阵正交性的约束下，最小化类内变换后的距离加权和，最大化类间变换后的距离加权和，以捕捉数据流形的局部结构，拉近相似点，推远不相似点。\n",
    "  - **约束条件**：保持变换矩阵 $W$ 的正交性（$W^T W = I$），加权矩阵 $s$ 的各行元素和为1且非负（$\\sum_{k=1}^{n_i} s_{jk}^i = 1, s_{jk}^i \\geq 0$）。\n",
    "\n",
    "- **LADA优势**：\n",
    "    - 它在不对数据分布做任何假设的情况下找到主要投影方向；\n",
    "    - 它能够在所需子空间中利用数据的局部流形结构；\n",
    "    - 它能自动利用点的邻近关系，而无需引入任何需要调优的附加参数。\n",
    "\n",
    "- **要点归纳**：\n",
    "    位置自适应判别分析（Locality Adaptive Discriminant Analysis，LADA）算法是一种新型的监督降维方法，旨在解决传统线性判别分析（LDA）在处理非高斯分布数据和忽略局部数据结构时的局限性。LADA通过捕捉数据流形的局部结构，实现了对相似点的拉近和对不相似点的推远，以下是LADA算法的具体描述：\n",
    "    1. **算法目标**:\n",
    "   \n",
    "        LADA算法的目标是学习一个最优变换矩阵 $W$，使得在降维后的子空间中：\n",
    "        - 相似的数据点更接近。\n",
    "        - 不相似的数据点更远离。\n",
    "    2. **目标函数**:\n",
    "   \n",
    "        给定数据点 $\\mathcal{X}=[x_1, x_2, \\cdots, x_n]，x_j \\in \\mathbb{R}^{d \\times 1}$，LADA的目标函数定义为：\n",
    "        $$\n",
    "        \\min_{W, s} \\frac{\\sum_{i=1}^{C} n_i \\sum_{j=1}^{n_i} \\sum_{k=1}^{n_i} s_{jk}^i{}^2 \\left\\|W^T(x_j^i - x_k^i)\\right\\|_2^2}{\\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} \\left\\|W^T(x_j - x_k)\\right\\|_2^2} \\\\\n",
    "        \\text{s.t.} \\quad W^T W = I, \\sum_{k=1}^{n_i} s_{jk}^i = 1, s_{jk}^i \\geq 0,\n",
    "        $$\n",
    "        其中，$n$ 是样本数量，$s$ 是加权矩阵，$s_{jk}^i$ 表示第 $i$ 类中第 $j$ 个样本和第 $k$ 个样本之间的权重。\n",
    "    3. **优化策略**\n",
    "\n",
    "        LADA采用一种自适应学习策略，通过迭代优化变换矩阵 $W$ 和加权矩阵 $s$ 来求解目标函数。\n",
    "        i. **初始化**：\n",
    "        - 将类 $i$ 中点的权重初始化为 $\\frac{1}{n_i}$，不同类的点权重设为0。\n",
    "        \n",
    "        ii. **迭代优化**：\n",
    "\n",
    "        - 固定 $s$，定义类内散布矩阵 $\\tilde{S}_w$ 和总散布矩阵 $\\tilde{S}_t$：\n",
    "            $$\n",
    "            \\tilde{S}_t = \\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} (x_j - x_k)(x_j - x_k)^T,\n",
    "            $$\n",
    "            $$\n",
    "            \\tilde{S}_w = \\sum_{i=1}^{C} n_i \\sum_{j=1}^{n_i} \\sum_{k=1}^{n_i} s_{jk}^i{}^2 (x_j^i - x_k^i)(x_j^i - x_k^i)^T.\n",
    "            $$\n",
    "        - 优化目标函数：\n",
    "            $$\n",
    "            \\min_{W^T W = I} \\frac{\\text{tr}(W^T \\tilde{S}_w W)}{\\text{tr}(W^T \\tilde{S}_t W)}.\n",
    "            $$\n",
    "        - 固定 $W$，优化 $s$：\n",
    "            $$\n",
    "            \\min_{s} \\sum_{i=1}^{C} \\sum_{j=1}^{n_i} \\sum_{k=1}^{n_i} s_{jk}^i{}^2 \\|W^T(x_j^i - x_k^i)\\|_2^2 \\\\\n",
    "            \\text{s.t.} \\sum_{k=1}^{n_i} s_{jk}^i = 1, s_{jk}^i \\geq 0.\n",
    "            $$\n",
    "            这相当于优化：\n",
    "            $$\n",
    "            \\min_{\\alpha^T \\mathbf{1} = 1, \\alpha \\geq 0} \\alpha^T V \\alpha,\n",
    "            $$\n",
    "            其中 $V_{kk} = \\|W^T(x_j^i - x_k^i)\\|_2^2$。\n",
    "    4. **算法优点**:\n",
    "\n",
    "       - **无分布假设**：LADA在找到主要投影方向时不对数据分布做任何假设。\n",
    "       - **局部结构利用**：LADA能够在所需子空间中利用数据的局部流形结构。\n",
    "       - **自动加权**：LADA自动利用点的邻近关系，而无需引入任何需要调优的附加参数。\n",
    "       - **避免小样本问题**：LADA的学习算法不需要计算类内散布矩阵的逆矩阵，因此避免了小样本问题。\n",
    "    5. **算法收敛**:\n",
    "   \n",
    "        在每次迭代中，目标函数单调减少，最终收敛到下界。此外，由于类间散布矩阵 $\\tilde{S}_b$ 是满秩的，LADA不存在过度降维问题。\n",
    "    6. **总结**:\n",
    "   \n",
    "        LADA是一种有效的监督降维方法，通过迭代优化变换矩阵和加权矩阵，可以在所需子空间中准确捕捉数据的局部关系，提高分类性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 算法流程\n",
    "\n",
    "在实际应用中，例如人脸分类，输入数据可能是多模态分布的。因此，捕捉数据流形的局部结构是至关重要的。我们的目标是学习一个最优变换矩阵 $W$，将相似点拉近，同时将不相似点推远。\n",
    "给定数据点 $\\mathcal{X}=[x_1,x_2,\\cdots,x_n],x_j\\in\\mathbb{R}^{d\\times1}$，目标函数定义为\n",
    "(7)\n",
    "$$\n",
    "\\min_{W,s} \\frac{\\sum_{i=1}^{C} n_{i} \\sum_{j=1}^{n_{i}} \\sum_{k=1}^{n_{i}} s_{jk}^{i}{}^{2}\\left\\|W^{T}(x_{j}^{i}-x_{k}^{i})\\right\\|_{2}^{2}}{\\frac{1}{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} \\left\\|W^{T}(x_{j}-x_{k})\\right\\|_{2}^{2}} \\\\\n",
    "\\text{s.t.} \\quad W^{T}W=I, \\sum_{k=1}^{n_{i}} s_{jk}^{i}=1, s_{jk}^{i}\\geq 0,\n",
    "$$\n",
    "其中，$n$ 是样本数量，$s$ 是加权矩阵，$s_{jk}^i$ 表示第 $i$ 类中第 $j$ 个样本和第 $k$ 个样本之间的权重。其余定义与LDA中的相同。注意，$x_j$是整个数据集中第$j$个样本，它不同于$x_j^i$。\n",
    "在问题(7)中，引入加权矩阵$s$来捕捉数据点之间的局部关系。对$s$的约束避免了$s$的某些行全为零的情况。假设已经获得变换矩阵$W$，如果变换后的距离$||W^T(x_j^i-x_k^i)||_2^2$很小，则$s_{jk}^i$会很大，这意味着$x_j^i$和$x_k^i$在学习到的子空间中是相似的。在下一步中，如果我们固定$s$并再次优化$W$，则目标函数将强调先前学习到的子空间中的相似点。因此，通过迭代优化$s$和$\\hat{W}$，可以学习到所需子空间中的点关系。\n",
    "\n",
    "接下来提出了一种自适应学习策略来解决问题(7)。首先，初始化类$i$中点的权重为$\\frac{1}{n_i}$，不同类的点权重设为0。然后，通过迭代求解$W$和$s$可以计算出最优解。\n",
    "当$s$固定时，定义$\\tilde{S}_{t}$和$\\tilde{S}_{w}$为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\tilde{S}_{t} = \\frac{1}{n}\\sum_{j=1}^{n}\\sum_{k=1}^{n}(x_{j}-x_{k})(x_{j}-x_{k})^{T}, \\\\\n",
    "&\\tilde{S}_{w} = \\sum_{i=1}^{C}n_{i}\\sum_{j=1}^{n_{i}}\\sum_{k=1}^{n_{i}}s_{jk}^{i}{}^{2}(x_{j}^{i}-x_{k}^{i})(x_{j}^{i}-x_{k}^{i})^{T},\n",
    "\\end{aligned}\n",
    "$$\n",
    "则问题(7)变为：\n",
    "(10)\n",
    "$$\n",
    "\\min_{W^TW=I}\\frac{\\text{tr}(W^T\\tilde{S}_wW)}{\\text{tr}(W^T\\tilde{S}_tW)},\n",
    "$$\n",
    "其中，$\\text{tr}()$表示迹运算。上述迹比问题可以通过[Nie et al., 2007]中的优化算法高效求解。\n",
    "当$W$固定时，目标函数(7)可以简化为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{s}\\sum_{i=1}^{C}\\sum_{j=1}^{n_{i}}\\sum_{k=1}^{n_{i}}s_{jk}^{i}{}^{2}||W^{T}(x_{j}^{i}-x_{k}^{i})||_{2}^{2}\\\\\n",
    "\\text{s.t.} \\sum_{k=1}^{n_{i}}s_{jk}^{i}=1, s_{jk}^{i}\\geq0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "(11)\n",
    "这相当于以下问题：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{s_{j}^{i}}\\sum_{k=1}^{n_{i}}{s_{jk}^{i}}^{2}||W^{T}(x_{j}^{i}-x_{k}^{i})||_{2}^{2}\\\\\n",
    "\\text{s.t.} \\sum_{k=1}^{n_{i}}s_{jk}^{i}=1, s_{jk}^{i}\\geq0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "(12)\n",
    "其中，$s_j^i$是一个列向量，其第$k$个元素等于$s_{jk}^{i}$。定义一个列向量$\\alpha$等于$s_j^{i}$，并定义$v_k$等于$||W^{T}(x_{j}^{i}-x_{k}^{i})||_{2}^{2}$，上述问题简化为：\n",
    "(13)\n",
    "$$\n",
    "\\min_{\\alpha^T\\mathbf{1}=1,\\alpha\\geq0}\\sum_{k=1}^{n_i}\\alpha_k^2v_k.\n",
    "$$\n",
    "\n",
    "定义一个对角矩阵 $ V $，其中 $ V_{kk} $ 等于 $ v_k $，问题 (13) 变为：\n",
    "(14)\n",
    "$$\n",
    "\\min_{\\alpha^T1=1,\\alpha\\geq0} \\alpha^T V \\alpha.\n",
    "$$\n",
    "没有第二个约束 $\\alpha \\geq 0$ 时，问题 (14) 的拉格朗日函数为：\n",
    "$$\n",
    "\\mathcal{L}(\\alpha,\\eta) = \\alpha^T V \\alpha - \\eta(\\alpha^T1 - 1),\n",
    "$$\n",
    "(15)\n",
    "其中 $\\eta$ 是拉格朗日乘数。对 Eq. (15) 关于 $\\alpha$ 求导并设其为零，我们得到：\n",
    "(16)\n",
    "$$\n",
    "2V\\alpha - \\eta1 = 0.\n",
    "$$\n",
    "结合约束 $\\alpha^T1 = 1$，可以计算出 $\\alpha$ 为：\n",
    "(17)\n",
    "$$\n",
    "\\alpha_k = \\frac{1}{v_k} \\times \\left(\\sum_{t=1}^{n_i}\\frac{1}{v_t}\\right)^{-1}.\n",
    "$$\n",
    "幸运的是，上述 $\\alpha$ 满足约束 $\\alpha \\geq 0$，因此它也是问题 (14) 的最优解。因此，问题 (11) 的最优解是：\n",
    "$$\n",
    "s_{jk}^i = \\frac{1}{||W^T(x_j^i - x_k^i)||_2^2} \\times \\left(\\sum_{t=1}^{n_i} \\frac{1}{||W^T(x_j^i - x_t^i)||_2^2}\\right)^{-1}.\n",
    "$$\n",
    "通过迭代优化 $ W $ 和 $ s $，我们的方法能够在所需子空间中量化数据点的局部关系。与现有的局部感知算法不同，我们的方法完全是自加权的，省去了调参的工作。\n",
    "目标在每次迭代中单调减少，最终收敛到下界。此外，对于 LADA，不存在过度降维问题，因为 $\\tilde{S}_b$ 是满秩的。而且我们的学习算法不需要计算 $\\tilde{S}_w$ 的逆矩阵，因此也避免了小样本问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 摘译\n",
    "\n",
    "#### 3.1. 摘要\n",
    "\n",
    "线性判别分析（LDA）是一种流行的监督降维技术，在处理高斯分布数据时表现令人满意。然而，由于忽略了数据的局部结构，使得LDA在许多实际情况下不适用。因此，一些工作集中在邻近点之间的判别分析上，这容易受到原始数据空间中噪声的影响。在本文中，我们提出了一种新的监督降维方法，位置自适应判别分析（LADA），以学习数据的代表性子空间。与LDA及其变体相比，该方法具有三个显著优势：(1) 它在不对数据分布做任何假设的情况下找到主要投影方向；(2) 它能够在所需子空间中利用数据的局部流形结构；(3) 它能自动利用点的邻近关系，而无需引入任何需要调优的附加参数。对合成数据集和实际基准数据集的性能测试表明了该方法的优越性。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

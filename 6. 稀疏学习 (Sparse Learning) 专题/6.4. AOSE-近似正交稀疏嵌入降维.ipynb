{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "摘要—局部线性嵌入（LLE）是最著名的流形学习方法之一。作为LLE的代表性线性扩展，正交邻域保持投影（ONPP）在降维领域引起了广泛关注。在本文中，\n",
    "\n",
    "- 通过引入稀疏性或L1范数学习，提出了一个统一的稀疏学习框架，进一步将基于LLE的方法扩展到稀疏情况。\n",
    "\n",
    "发现了ONPP与所提出的稀疏线性嵌入之间的理论联系。从所提框架派生的最优稀疏嵌入可以通过迭代修改的弹性网和奇异值分解来计算。我们还展示了所提模型可以被视为稀疏线性和非线性（核）子空间学习的通用模型。基于这个通用模型，还提出了稀疏核嵌入用于非线性稀疏特征提取。在五个数据库上的广泛实验表明，所提出的稀疏学习框架比现有的子空间学习算法表现更好，特别是在小样本大小的情况下。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}&\\min_{B,P}\\sum_{i}\\|BP^{T}XM_{i,:}^{T}-XM_{i,:}^{T}\\|_{F}^{2}+\\beta\\|P\\|_{F}^{2}+\\sum_{l=1}^{d}\\gamma_{l}|p_{l}|\\\\&\\mathrm{s.t.}\\quad B^{T}B=I_{d}&\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "这个损失函数是一个多目标优化问题，结合了矩阵分解和正则化技术，目标是在满足一定约束的情况下，找到两个矩阵 $B$ 和 $P$，使得给定的损失函数最小化。具体来说，这个函数包含了三个主要部分：重构误差、正则化项和约束条件。我们一步步分析如下：\n",
    "### 1. 重构误差\n",
    "$$ \\sum_{i} \\|BP^T X M_{i,:}^T - X M_{i,:}^T\\|_F^2 $$\n",
    "这部分目标是最小化原始数据 $X$ 的某些变换（通过 $M_{i,:}$ 选择的行）与通过矩阵 $B$ 和 $P$ 变换后数据之间的Frobenius范数误差。这实际上是在尝试通过 $B$ 和 $P$ 的乘积来近似原始数据 $X$ 的一个子集，其中 $M_{i,:}$ 可能代表了数据的一部分特征或样本。\n",
    "### 2. 正则化项\n",
    "这个损失函数包括两种正则化：\n",
    "- **Frobenius范数正则化**：$$ \\beta \\|P\\|_F^2 $$\n",
    "  这一项对矩阵 $P$ 进行正则化，防止过拟合，并且控制 $P$ 的大小，使其不会变得过大，从而保持模型的泛化能力。\n",
    "- **L1范数正则化**：$$ \\sum_{l=1}^d \\gamma_l |p_l| $$\n",
    "  这一项对矩阵 $P$ 的每一行进行L1正则化，增加稀疏性。这意味着很多 $p_l$ 的元素会变为零，这样的稀疏性可以帮助在特征选择上得到更有解释性的模型。每个 $p_l$ 可能对应 $P$ 中的一个特征，而 $\\gamma_l$ 是调整这些特征稀疏性的正则化参数。\n",
    "### 3. 约束条件\n",
    "$$ B^T B = I_d $$\n",
    "这个约束确保了矩阵 $B$ 是正交的（orthogonal）。正交约束通常用于保持变换后的特征的独立性，避免冗余。这对于特征提取和降维是有益的，因为它保持了特征之间的独立性。\n",
    "### 结合分析\n",
    "此优化问题通过结合重构误差最小化与正则化策略，在满足正交约束的同时，致力于寻找能够最好地重建原始数据 $X$ 的矩阵 $B$ 和 $P$。其中，正则化部分帮助控制模型复杂性，避免过拟合，并通过稀疏性增强模型的解释性。整体而言，这是一个复杂的优化问题，可能需要高级的优化技术来求解。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
